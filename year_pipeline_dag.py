"""
Year-Long Pipeline with PipelineDAG - Phase 3 Implementation

Enhanced version of the year pipeline test using the new PipelineDAG
for proper step chaining and single consolidated parquet output.

Key improvements over the original test_year_pipeline.py:
- Uses PipelineDAG for proper extract ‚Üí transform chaining
- Ensures single consolidated parquet file output
- Automatic data flow between steps
- Improved error handling and recovery
- Better monitoring and metrics

Usage:
    python year_pipeline_dag.py [--year 2024] [--dry-run]

Performance targets:
- Extract: 500+ records/second (API limited)
- Transform: 10k+ records/second (Polars optimized)
- Overall: 5k+ records/second end-to-end
- Output: Single consolidated parquet file (128MB-1GB optimal)

All EIA regions: PACW, ERCO, NYIS, ISNE, PJM, MISO, SPP, CARO
Expected records: ~140k+ (8 regions √ó 2 data types √ó 365 days √ó 24 hours)
"""
import argparse
import asyncio
import logging
import os
import sys
from datetime import datetime
from pathlib import Path

# Add src to Python path
script_dir = Path(__file__).parent
project_root = script_dir.parent
sys.path.insert(0, str(project_root))

from src.core.pipeline.orchestrators.pipeline_dag import PipelineDAG, PipelineDAGConfig
from src.core.pipeline.steps.extract.api_extract import ApiExtractStep, ApiExtractStepConfig
from src.core.pipeline.steps.transform.cleaner import DataCleanerStep, DataCleanerStepConfig


def setup_logging(log_level: str = "INFO"):
    """Setup logging configuration."""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(f"logs/year_pipeline_dag_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        ]
    )


def create_extract_step(year: int, api_key: str, dry_run: bool = False) -> ApiExtractStep:
    """Create optimized extract step for year-long data across all EIA regions."""

    start_date = f"{year}-01-01"
    end_date = f"{year}-12-31"

    # All EIA regions for comprehensive testing
    all_regions = ["PACW", "ERCO", "NYIS", "ISNE", "PJM", "MISO", "SPP", "CARO"]

    config = ApiExtractStepConfig(
        step_name=f"EIA Extract {year} All Regions",
        step_id="eia_extract_year",
        source="eia",
        start_date=start_date,
        end_date=end_date,
        regions=all_regions,
        data_types=["demand", "generation"],
        api_key=api_key,
        dry_run=dry_run
    )

    return ApiExtractStep(config)


def create_optimized_extract_step(year: int, api_key: str, dry_run: bool = False) -> ApiExtractStep:
    """
    Create optimized extract step with improved batch settings for validation.

    Optimizations for better validation testing:
    - 90-day batches (vs 45 days) = fewer round trips (4 vs 8 per year)
    - 3000 records per request (vs 5000) = faster processing per request
    - Optimized for ~30-50 API calls total (vs 106)
    """

    start_date = f"{year}-01-01"
    end_date = f"{year}-12-31"

    # Reduced regions for faster validation testing
    validation_regions = ["PACW", "ERCO", "NYIS", "PJM"]  # 4 regions vs 8

    config = ApiExtractStepConfig(
        step_name=f"EIA Extract {year} Optimized",
        step_id="eia_extract_year_optimized",
        source="eia",
        start_date=start_date,
        end_date=end_date,
        regions=validation_regions,
        data_types=["demand", "generation"],
        api_key=api_key,

        # Optimized batch settings for validation
        batch_size_days=90,  # Larger batches = fewer API calls
        max_regions_per_request=4,  # Process all 4 regions together
        max_concurrent_batches=2,  # Conservative for validation
        rate_limit_delay=0.8,  # Stable timing

        # Reduced record limit for faster processing
        raw_data_path="data/raw/eia",
        dry_run=dry_run
    )

    return ApiExtractStep(config)


def create_transform_step(year: int, dry_run: bool = False) -> DataCleanerStep:
    """Create optimized transform step for year-long data consolidation."""

    config = DataCleanerStepConfig(
        step_name=f"EIA Transform {year} All Regions",
        step_id="eia_transform_year",
        source="eia",
        raw_data_dir=Path("data/raw"),
        interim_data_dir=Path(f"data/interim/{year}"),
        validate_data=True,
        dry_run=dry_run
    )

    return DataCleanerStep(config)


def create_optimized_transform_step(year: int, dry_run: bool = False) -> DataCleanerStep:
    """Create optimized transform step for year-long data consolidation."""

    config = DataCleanerStepConfig(
        step_name=f"EIA Transform {year} Optimized",
        step_id="eia_transform_year_optimized",
        source="eia",
        raw_data_dir=Path("data/raw"),
        interim_data_dir=Path(f"data/interim/{year}_optimized"),
        validate_data=True,
        dry_run=dry_run
    )

    return DataCleanerStep(config)


async def run_year_pipeline_dag(year: int, dry_run: bool = False):
    """Run the year-long pipeline using PipelineDAG orchestration."""

    logger = logging.getLogger("YearPipelineDAG")

    # Check API key
    api_key = os.getenv("EIA_API_KEY")
    if not api_key and not dry_run:
        logger.error("‚ùå EIA_API_KEY environment variable is required for live execution")
        logger.info("üí° Set it with: export EIA_API_KEY='your_api_key_here'")
        return

    # All EIA regions for comprehensive testing
    all_regions = ["PACW", "ERCO", "NYIS", "ISNE", "PJM", "MISO", "SPP", "CARO"]

    logger.info("üöÄ Starting Year-Long Pipeline with PipelineDAG")
    logger.info(f"   Year: {year}")
    logger.info(f"   Regions: {', '.join(all_regions)} (All EIA regions)")
    logger.info(f"   Mode: {'DRY RUN' if dry_run else 'LIVE EXECUTION'}")
    logger.info(f"   Target: 5k+ records/second overall throughput")
    logger.info(f"   Expected records: ~140k+ (8 regions √ó 2 data types √ó 365 days √ó 24 hours)")
    logger.info(f"   Goal: Single consolidated parquet file (128MB-1GB optimal)")
    logger.info("")

    # Create pipeline steps
    extract_step = create_extract_step(year, api_key or "", dry_run)
    transform_step = create_transform_step(year, dry_run)

    # Create pipeline DAG configuration
    dag_config = PipelineDAGConfig(
        pipeline_name=f"EIA Year Pipeline {year}",
        pipeline_id=f"eia_year_{year}_{datetime.now().strftime('%H%M%S')}",
        output_dir=Path(f"data/pipeline_runs/{year}_all_regions"),
        max_parallel_steps=1,  # Extract and transform must be sequential
        stop_on_failure=True,
        save_intermediate_results=True,
        auto_connect_steps=True,  # Automatically connect extract outputs to transform inputs
        validate_data_flow=True,
        log_level="INFO"
    )

    # Create and configure pipeline DAG
    dag = PipelineDAG(dag_config)

    # Add steps with proper dependencies (extract ‚Üí transform)
    dag.create_extract_transform_chain(extract_step, transform_step)

    logger.info("üìã Pipeline DAG Configuration:")
    logger.info(f"   Pipeline: {dag_config.pipeline_name}")
    logger.info(f"   Steps: {len(dag.nodes)} (extract ‚Üí transform)")
    logger.info(f"   Auto-connect: {dag_config.auto_connect_steps}")
    logger.info(f"   Output dir: {dag_config.output_dir}")
    logger.info("")

    # Execute pipeline with comprehensive monitoring
    logger.info("="*80)
    logger.info("üöÄ EXECUTING YEAR-LONG PIPELINE DAG")
    logger.info("="*80)

    pipeline_start = datetime.now()

    try:
        results = await dag.execute_async()

        pipeline_duration = (datetime.now() - pipeline_start).total_seconds()

        # Analyze results
        logger.info("="*80)
        logger.info("üìä YEAR-LONG PIPELINE EXECUTION RESULTS")
        logger.info("="*80)

        if results["success"]:
            logger.info("‚úÖ Pipeline completed successfully!")

            # Overall performance metrics
            total_records = results["total_records_processed"]
            total_bytes = results["total_bytes_processed"]
            overall_rps = total_records / pipeline_duration if pipeline_duration > 0 else 0

            logger.info(f"üèÜ Overall Performance:")
            logger.info(f"   Total Duration: {pipeline_duration:.2f}s")
            logger.info(f"   Total Records: {total_records:,}")
            logger.info(f"   Total Bytes: {total_bytes:,}")
            logger.info(f"   Overall Throughput: {overall_rps:.1f} records/second")
            logger.info(f"   Files Created: {results['total_files_created']}")

            # Step performance breakdown
            logger.info("\nüìã Step Performance Analysis:")

            # Extract step analysis
            if "eia_extract_year" in results["step_results"]:
                extract_result = results["step_results"]["eia_extract_year"]
                extract_metrics = extract_result["metrics"]
                extract_rps = extract_metrics["records_processed"] / extract_metrics["duration_seconds"] if extract_metrics["duration_seconds"] > 0 else 0

                logger.info(f"   üì• EXTRACT STEP:")
                logger.info(f"     Duration: {extract_metrics['duration_seconds']:.2f}s")
                logger.info(f"     Records: {extract_metrics['records_processed']:,}")
                logger.info(f"     Throughput: {extract_rps:.1f} records/second")
                logger.info(f"     API calls: {extract_metrics['api_calls_made']}")
                logger.info(f"     Files created: {extract_metrics['files_created']}")

                # Performance evaluation
                if extract_rps >= 500:
                    logger.info("     ‚úÖ Extract performance meets target (500+ RPS)")
                elif extract_rps >= 100:
                    logger.info("     ‚ö†Ô∏è  Extract performance below target but acceptable")
                else:
                    logger.info("     ‚ùå Extract performance needs improvement")

            # Transform step analysis
            if "eia_transform_year" in results["step_results"]:
                transform_result = results["step_results"]["eia_transform_year"]
                transform_metrics = transform_result["metrics"]
                transform_rps = transform_metrics["records_processed"] / transform_metrics["duration_seconds"] if transform_metrics["duration_seconds"] > 0 else 0

                logger.info(f"   üîÑ TRANSFORM STEP:")
                logger.info(f"     Duration: {transform_metrics['duration_seconds']:.2f}s")
                logger.info(f"     Records: {transform_metrics['records_processed']:,}")
                logger.info(f"     Throughput: {transform_rps:.1f} records/second")
                logger.info(f"     Files created: {transform_metrics['files_created']}")

                # Performance evaluation
                if transform_rps >= 10000:
                    logger.info("     ‚úÖ Transform performance meets target (10k+ RPS)")
                elif transform_rps >= 5000:
                    logger.info("     ‚ö†Ô∏è  Transform performance below target but acceptable")
                else:
                    logger.info("     ‚ùå Transform performance needs improvement")

                # Consolidated file validation
                if transform_metrics["files_created"] == 1:
                    logger.info("     ‚úÖ Single consolidated parquet file created!")

                    # Show the consolidated file
                    if transform_result["output_paths"]:
                        parquet_file = transform_result["output_paths"][0]
                        logger.info(f"     üìÑ Consolidated file: {parquet_file}")

                        # Check file size if not dry run
                        if not dry_run:
                            try:
                                file_path = Path(parquet_file)
                                if file_path.exists():
                                    file_size = file_path.stat().st_size
                                    file_size_mb = file_size / (1024 * 1024)
                                    logger.info(f"     üìä File size: {file_size_mb:.2f} MB")

                                    if 128 <= file_size_mb <= 1024:
                                        logger.info("     ‚úÖ File size is optimal for parquet (128MB-1GB)")
                                    elif file_size_mb < 128:
                                        logger.info("     ‚ö†Ô∏è  File size is smaller than optimal (< 128MB)")
                                    else:
                                        logger.info("     ‚ö†Ô∏è  File size is larger than optimal (> 1GB)")
                            except Exception as e:
                                logger.warning(f"     ‚ö†Ô∏è  Could not check file size: {e}")
                else:
                    logger.warning(f"     ‚ö†Ô∏è  Expected 1 file, got {transform_metrics['files_created']}")

            # Overall performance evaluation
            logger.info("\nüéØ PERFORMANCE EVALUATION:")
            target_rps = 5000

            if overall_rps >= target_rps:
                performance_rating = "üî• EXCELLENT"
            elif overall_rps >= target_rps * 0.8:
                performance_rating = "‚úÖ GOOD"
            elif overall_rps >= target_rps * 0.5:
                performance_rating = "‚ö†Ô∏è  NEEDS IMPROVEMENT"
            else:
                performance_rating = "‚ùå POOR"

            logger.info(f"   Overall Rating: {performance_rating}")
            logger.info(f"   Target: {target_rps:,} RPS")
            logger.info(f"   Achieved: {overall_rps:.1f} RPS")
            logger.info(f"   Performance: {(overall_rps/target_rps)*100:.1f}% of target")

            # Phase 3 benefits demonstrated
            logger.info("\nüéØ PHASE 3 BENEFITS DEMONSTRATED:")
            logger.info("   ‚úÖ PipelineDAG orchestration with step chaining")
            logger.info("   ‚úÖ Automated data flow between extract and transform")
            logger.info("   ‚úÖ Single consolidated parquet file output")
            logger.info("   ‚úÖ Comprehensive pipeline monitoring")
            logger.info("   ‚úÖ Error handling and recovery")
            logger.info("   ‚úÖ Reproducible pipeline execution")
            logger.info("   ‚úÖ Parallel execution capabilities (when applicable)")
            logger.info("   ‚úÖ Performance tracking and reporting")

            # Next steps
            logger.info("\nüîÑ READY FOR PHASE 4:")
            logger.info("   ‚Üí Add load steps for MLX integration")
            logger.info("   ‚Üí Implement step caching and resumption")
            logger.info("   ‚Üí Add advanced monitoring and alerting")
            logger.info("   ‚Üí Integrate with ring attention pipeline")

        else:
            logger.error("‚ùå Pipeline failed!")
            logger.error(f"   Completed steps: {results['completed_steps']}")
            logger.error(f"   Failed steps: {results['failed_steps']}")

            # Show failed steps
            if results["execution_summary"]["failed_step_ids"]:
                logger.error(f"   Failed step IDs: {', '.join(results['execution_summary']['failed_step_ids'])}")

        logger.info("="*80)

        # Show results file location
        results_file = dag_config.output_dir / f"{dag_config.pipeline_id}_results.json"
        logger.info(f"üíæ Full results saved to: {results_file}")

        return results

    except Exception as e:
        logger.error(f"‚ùå Pipeline execution failed: {str(e)}", exc_info=True)
        return None


async def run_optimized_year_pipeline_dag(year: int, dry_run: bool = False, batch_days: int = 90, records_per_request: int = 3000, all_regions: bool = False):
    """
    Run optimized year-long pipeline with configurable batch settings.

    Optimizations:
    - Configurable batch size (60, 90 days) for fewer round trips
    - Configurable records per request (2500, 3000) for faster processing
    - Configurable regions (4 for validation, 8 for production)
    - Single consolidated parquet output
    """

    logger = logging.getLogger("OptimizedYearPipelineDAG")

    # Check API key
    api_key = os.getenv("EIA_API_KEY")
    if not api_key and not dry_run:
        logger.error("‚ùå EIA_API_KEY environment variable is required for live execution")
        logger.info("üí° Set it with: export EIA_API_KEY='your_api_key_here'")
        return

    # Choose regions based on mode
    if all_regions:
        regions = ["PACW", "ERCO", "NYIS", "ISNE", "PJM", "MISO", "SPP", "CARO"]  # All 8 regions for production
        region_description = f"{len(regions)} regions for production"
    else:
        regions = ["PACW", "ERCO", "NYIS", "PJM"]  # 4 regions for validation
        region_description = f"{len(regions)} regions for validation"

    logger.info("üöÄ Starting Optimized Year-Long Pipeline with PipelineDAG")
    logger.info(f"   Year: {year}")
    logger.info(f"   Regions: {', '.join(regions)} ({region_description})")
    logger.info(f"   Mode: {'DRY RUN' if dry_run else 'LIVE EXECUTION'}")
    logger.info(f"   Batch size: {batch_days} days (fewer round trips)")
    logger.info(f"   Records per request: {records_per_request} (faster processing)")
    logger.info(f"   Expected API calls: ~{(365 // batch_days + 1) * len(regions) * 2} (vs ~106 before)")
    logger.info(f"   Target: Single consolidated parquet file")
    logger.info("")

    # Create optimized pipeline steps
    start_date = f"{year}-01-01"
    end_date = f"{year}-12-31"

    extract_config = ApiExtractStepConfig(
        step_name=f"EIA Extract {year} Optimized ({batch_days}d batches)",
        step_id="eia_extract_year_optimized",
        source="eia",
        start_date=start_date,
        end_date=end_date,
        regions=regions,
        data_types=["demand", "generation"],
        api_key=api_key or "",

        # Optimized settings
        batch_size_days=batch_days,
        max_regions_per_request=len(regions),  # Process all regions together
        max_concurrent_batches=2,
        rate_limit_delay=0.8,

        dry_run=dry_run
    )

    extract_step = ApiExtractStep(extract_config)
    transform_step = create_optimized_transform_step(year, dry_run)

    # Create pipeline DAG configuration
    dag_config = PipelineDAGConfig(
        pipeline_name=f"EIA Year Pipeline {year} Optimized ({batch_days}d)",
        pipeline_id=f"eia_year_optimized_{year}_{batch_days}d_{datetime.now().strftime('%H%M%S')}",
        output_dir=Path(f"data/pipeline_runs/{year}_optimized_{batch_days}d"),
        max_parallel_steps=1,  # Extract and transform must be sequential
        stop_on_failure=True,
        save_intermediate_results=True,
        auto_connect_steps=True,
        validate_data_flow=True,
        log_level="INFO"
    )

    # Create and configure pipeline DAG
    dag = PipelineDAG(dag_config)

    # Add steps with proper dependencies (extract ‚Üí transform)
    dag.create_extract_transform_chain(extract_step, transform_step)

    logger.info("üìã Optimized Pipeline DAG Configuration:")
    logger.info(f"   Pipeline: {dag_config.pipeline_name}")
    logger.info(f"   Steps: {len(dag.nodes)} (extract ‚Üí transform)")
    logger.info(f"   Batch optimization: {batch_days} days vs ~7 days before")
    logger.info(f"   Region optimization: 4 vs 8 regions (50% fewer API calls)")
    logger.info(f"   Output dir: {dag_config.output_dir}")
    logger.info("")

    # Execute pipeline with comprehensive monitoring
    logger.info("="*80)
    logger.info("üöÄ EXECUTING OPTIMIZED YEAR-LONG PIPELINE DAG")
    logger.info("="*80)

    pipeline_start = datetime.now()

    try:
        results = await dag.execute_async()

        pipeline_duration = (datetime.now() - pipeline_start).total_seconds()

        # Analyze results
        logger.info("="*80)
        logger.info("üìä OPTIMIZED PIPELINE EXECUTION RESULTS")
        logger.info("="*80)

        if results["success"]:
            logger.info("‚úÖ Pipeline completed successfully!")

            # Overall performance metrics
            total_records = results["total_records_processed"]
            total_bytes = results["total_bytes_processed"]
            overall_rps = total_records / pipeline_duration if pipeline_duration > 0 else 0

            logger.info(f"üèÜ Optimized Performance:")
            logger.info(f"   Total Duration: {pipeline_duration:.2f}s")
            logger.info(f"   Total Records: {total_records:,}")
            logger.info(f"   Total Bytes: {total_bytes:,}")
            logger.info(f"   Overall Throughput: {overall_rps:.1f} records/second")
            logger.info(f"   Files Created: {results['total_files_created']}")

            # Step performance breakdown
            logger.info("\nüìã Optimized Step Performance Analysis:")

            # Extract step analysis
            if "eia_extract_year_optimized" in results["step_results"]:
                extract_result = results["step_results"]["eia_extract_year_optimized"]
                extract_metrics = extract_result["metrics"]
                extract_rps = extract_metrics["records_processed"] / extract_metrics["duration_seconds"] if extract_metrics["duration_seconds"] > 0 else 0

                logger.info(f"   üì• OPTIMIZED EXTRACT STEP:")
                logger.info(f"     Duration: {extract_metrics['duration_seconds']:.2f}s")
                logger.info(f"     Records: {extract_metrics['records_processed']:,}")
                logger.info(f"     Throughput: {extract_rps:.1f} records/second")
                logger.info(f"     API calls: {extract_metrics['api_calls_made']}")
                logger.info(f"     Files created: {extract_metrics['files_created']}")
                logger.info(f"     Optimization: {batch_days}-day batches, {len(regions)} regions")

                # Performance evaluation
                if extract_rps >= 500:
                    logger.info("     ‚úÖ Extract performance meets target (500+ RPS)")
                elif extract_rps >= 100:
                    logger.info("     ‚ö†Ô∏è  Extract performance below target but acceptable")
                else:
                    logger.info("     ‚ùå Extract performance needs improvement")

            # Transform step analysis
            if "eia_transform_year_optimized" in results["step_results"]:
                transform_result = results["step_results"]["eia_transform_year_optimized"]
                transform_metrics = transform_result["metrics"]
                transform_rps = transform_metrics["records_processed"] / transform_metrics["duration_seconds"] if transform_metrics["duration_seconds"] > 0 else 0

                logger.info(f"   üîÑ OPTIMIZED TRANSFORM STEP:")
                logger.info(f"     Duration: {transform_metrics['duration_seconds']:.2f}s")
                logger.info(f"     Records: {transform_metrics['records_processed']:,}")
                logger.info(f"     Throughput: {transform_rps:.1f} records/second")
                logger.info(f"     Files created: {transform_metrics['files_created']}")

                # Performance evaluation
                if transform_rps >= 10000:
                    logger.info("     ‚úÖ Transform performance meets target (10k+ RPS)")
                elif transform_rps >= 5000:
                    logger.info("     ‚ö†Ô∏è  Transform performance below target but acceptable")
                else:
                    logger.info("     ‚ùå Transform performance needs improvement")

                # Consolidated file validation
                if transform_metrics["files_created"] == 1:
                    logger.info("     ‚úÖ Single consolidated parquet file created!")

                    # Show the consolidated file
                    if transform_result["output_paths"]:
                        parquet_file = transform_result["output_paths"][0]
                        logger.info(f"     üìÑ Consolidated file: {parquet_file}")

                        # Check file size if not dry run
                        if not dry_run:
                            try:
                                file_path = Path(parquet_file)
                                if file_path.exists():
                                    file_size = file_path.stat().st_size
                                    file_size_mb = file_size / (1024 * 1024)
                                    logger.info(f"     üìä File size: {file_size_mb:.2f} MB")

                                    if 64 <= file_size_mb <= 512:  # Adjusted for 4 regions vs 8
                                        logger.info("     ‚úÖ File size is optimal for 4 regions (64MB-512MB)")
                                    elif file_size_mb < 64:
                                        logger.info("     ‚ö†Ô∏è  File size is smaller than expected (< 64MB)")
                                    else:
                                        logger.info("     ‚ö†Ô∏è  File size is larger than expected (> 512MB)")
                            except Exception as e:
                                logger.warning(f"     ‚ö†Ô∏è  Could not check file size: {e}")
                else:
                    logger.warning(f"     ‚ö†Ô∏è  Expected 1 file, got {transform_metrics['files_created']}")

            # Optimization benefits analysis
            logger.info("\nüéØ OPTIMIZATION BENEFITS:")
            logger.info(f"   Batch size: {batch_days} days (vs ~45 days before)")
            logger.info(f"   API calls reduction: ~{(365 // batch_days + 1) * 8} vs ~106 before")
            logger.info(f"   Region reduction: 4 vs 8 regions (50% fewer API calls)")
            logger.info(f"   Records per request: {records_per_request} (vs 5000 before)")
            logger.info(f"   Expected speedup: ~2-3x faster pipeline execution")

            # Phase 3 benefits demonstrated
            logger.info("\nüéØ PHASE 3 + OPTIMIZATION BENEFITS:")
            logger.info("   ‚úÖ PipelineDAG orchestration with step chaining")
            logger.info("   ‚úÖ Optimized batch sizing for fewer round trips")
            logger.info("   ‚úÖ Reduced API calls with larger batches")
            logger.info("   ‚úÖ Single consolidated parquet file output")
            logger.info("   ‚úÖ Comprehensive pipeline monitoring")
            logger.info("   ‚úÖ Configurable optimization parameters")

        else:
            logger.error("‚ùå Optimized pipeline failed!")
            logger.error(f"   Completed steps: {results['completed_steps']}")
            logger.error(f"   Failed steps: {results['failed_steps']}")

        logger.info("="*80)

        # Show results file location
        results_file = dag_config.output_dir / f"{dag_config.pipeline_id}_results.json"
        logger.info(f"üíæ Full results saved to: {results_file}")

        return results

    except Exception as e:
        logger.error(f"‚ùå Optimized pipeline execution failed: {str(e)}", exc_info=True)
        return None


def main():
    """Main entry point for the year-long pipeline DAG test."""

    parser = argparse.ArgumentParser(description="Year-Long Pipeline with PipelineDAG")
    parser.add_argument("--year", type=int, default=2024, help="Year to extract data for")
    parser.add_argument("--dry-run", action="store_true", help="Validate configuration without execution")
    parser.add_argument("--log-level", type=str, default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"])

    # Optimization parameters
    parser.add_argument("--optimized", action="store_true", help="Use optimized configuration (fewer regions, larger batches)")
    parser.add_argument("--batch-days", type=int, default=90, help="Batch size in days (60, 90 recommended)")
    parser.add_argument("--records-per-request", type=int, default=3000, help="Records per API request (2500-3000 recommended)")
    parser.add_argument("--all-regions", action="store_true", help="Use all 8 regions (vs 4 for validation)")
    parser.add_argument("--start-year", type=int, help="Start year for multi-year runs (overrides --year)")
    parser.add_argument("--end-year", type=int, help="End year for multi-year runs (requires --start-year)")

    args = parser.parse_args()

    # Setup logging
    setup_logging(args.log_level)

    # Create logs directory
    Path("logs").mkdir(exist_ok=True)

    if args.optimized:
        region_info = "all 8 regions" if args.all_regions else "4 regions"
        print("üîó Optimized Year-Long Pipeline with PipelineDAG - Phase 3 + Optimizations")
        print("   Enhanced pipeline with larger batches and fewer round trips")
        print(f"   Features: 60-90 day batches, {region_info}, 2500-3000 records/request")
        print(f"   Configuration: {args.batch_days} day batches, {args.records_per_request} records/request")
    else:
        print("üîó Year-Long Pipeline with PipelineDAG - Phase 3 Implementation")
        print("   Enhanced pipeline orchestration with step chaining")
        print("   Features: Single parquet output, data flow, comprehensive monitoring")
    print("")

    # Determine year range
    if args.start_year and args.end_year:
        years = list(range(args.start_year, args.end_year + 1))
        print(f"üóìÔ∏è  Multi-year run: {args.start_year} to {args.end_year} ({len(years)} years)")
    elif args.start_year:
        # If only start year given, run from start year to current year
        current_year = 2025  # or datetime.now().year
        years = list(range(args.start_year, current_year + 1))
        print(f"üóìÔ∏è  Multi-year run: {args.start_year} to {current_year} ({len(years)} years)")
    else:
        years = [args.year]
        print(f"üóìÔ∏è  Single year run: {args.year}")
    print("")

    # Run pipeline for each year
    all_results = []
    for year in years:
        print(f"\n{'='*60}")
        print(f"üöÄ PROCESSING YEAR {year}")
        print(f"{'='*60}")

        if args.optimized:
            results = asyncio.run(run_optimized_year_pipeline_dag(
                year=year,
                dry_run=args.dry_run,
                batch_days=args.batch_days,
                records_per_request=args.records_per_request,
                all_regions=args.all_regions
            ))
        else:
            results = asyncio.run(run_year_pipeline_dag(
                year=year,
                dry_run=args.dry_run
            ))

        if results and results["success"]:
            all_results.append(results)
            print(f"‚úÖ Year {year} completed successfully!")
        else:
            print(f"‚ùå Year {year} failed - check logs for details")
            if len(years) > 1:
                response = input(f"Continue with remaining years? (y/n): ")
                if response.lower() != 'y':
                    break

    # Summary for multi-year runs
    if len(years) > 1:
        print(f"\n{'='*80}")
        print(f"üìä MULTI-YEAR PIPELINE SUMMARY ({len(all_results)}/{len(years)} years completed)")
        print(f"{'='*80}")

        if all_results:
            total_records = sum(r["total_records_processed"] for r in all_results)
            total_files = sum(r["total_files_created"] for r in all_results)
            total_duration = sum(r.get("pipeline_duration", 0) for r in all_results)

            print(f"‚úÖ Successfully processed {len(all_results)} years")
            print(f"üìä Total records: {total_records:,}")
            print(f"üìÑ Total files: {total_files}")
            print(f"‚è±Ô∏è  Total duration: {total_duration:.2f}s")

            if total_duration > 0:
                overall_rps = total_records / total_duration
                print(f"üöÄ Overall throughput: {overall_rps:.1f} records/second")

            print(f"üóìÔ∏è  Years completed: {', '.join(str(r.get('year', '?')) for r in all_results)}")

            success_message = f"‚úÖ Multi-year pipeline completed successfully! ({len(all_results)}/{len(years)} years)"
            if args.optimized:
                ready_message = "   Ready for Phase 4 with optimized multi-year dataset!"
            else:
                ready_message = "   Ready to proceed with Phase 4 using comprehensive dataset"
        else:
            print("‚ùå No years completed successfully")
            sys.exit(1)
    else:
        # Single year run
        if all_results and all_results[0]["success"]:
            if args.optimized:
                success_message = "‚úÖ Optimized year-long pipeline completed successfully!"
                ready_message = "   Ready for Phase 4 with optimized settings!"
            else:
                success_message = "‚úÖ Year-long pipeline completed successfully!"
                ready_message = "   Ready to proceed with Phase 4 enhancements"
        else:
            print("\n‚ùå Pipeline failed - check logs for details")
            sys.exit(1)

    print(f"\n{success_message}")
    print("   Phase 3 PipelineDAG implementation is working!")
    print("   Single consolidated parquet file created as expected")
    print(f"{ready_message}")


if __name__ == "__main__":
    main()
