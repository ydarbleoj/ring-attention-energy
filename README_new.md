# Ring Attention Energy Pipeline

**High-Performance Energy Data Pipeline + Ring Attention Research Platform**

This project builds a production-grade energy data extraction pipeline capable of **4,000+ RPS** and implements Ring Attention mechanisms for analyzing multi-year energy datasets. The system combines optimized data engineering with cutting-edge ML research.

## ğŸ¯ Project Goals

**Primary Objective**: Build a fast, reliable pipeline for energy data extraction and implement Ring Attention with Reinforcement Learning for energy analysis and ring attention research.

**Key Outcomes**:

- âœ… **High-Speed Data Pipeline**: 4,051 RPS sustained throughput
- ğŸ”¬ **Ring Attention Research**: Memory-efficient attention for long sequences
- ğŸ“Š **Energy Analysis**: ML models for grid forecasting and optimization
- ğŸš€ **Production Ready**: Complete 2019-2025 extraction in 5.1 minutes

## ğŸ“‹ Current Status

### âœ… Phase 1 Complete: Data Pipeline (July 2025)

- **Performance**: 4,051 RPS achieved (12.5x baseline improvement)
- **Coverage**: Complete 2019-2025 historical data (1.2M+ records)
- **Reliability**: 100% success rate across all test phases
- **Infrastructure**: Production-ready extraction system

### ğŸ”¬ Phase 2: Ring Attention Implementation

- Memory-efficient attention mechanism for energy sequences
- Multi-device distributed processing
- Long-sequence energy forecasting capabilities

### ğŸ§  Phase 3: RL Integration

- Reinforcement learning for energy optimization
- Grid stability and demand response modeling
- Ring attention + RL hybrid architectures

## ğŸš€ Quick Start

### Prerequisites

```bash
python 3.11+
pip install -r requirements.txt
```

### Run Full Historical Extraction

```bash
# Test mode (January 2024 only - ~30 seconds)
python scripts/run_full_extraction_monitored.py --test

# Full production extraction (2019-2025 - ~5 minutes)
python scripts/run_full_extraction_monitored.py
```

### Expected Output

```
ğŸš€ HISTORICAL DATA EXTRACTION
==================================================
ğŸ“… Period: 2019-01-01 to 2025-12-31
â±ï¸  Estimated time: 6-8 minutes
ğŸ”§ Mode: Full Production

âœ… SUCCESS!
ğŸ“ Files created: 548
ğŸ“Š Records: 1,248,900
â±ï¸  Duration: 308.2s (5.1 min)
ğŸš€ Average RPS: 4,051.9
```

## ğŸ—ï¸ Architecture

### Data Pipeline

```
EIA API â†’ ThreadPoolExecutor â†’ Raw JSON â†’ Processing â†’ Ring Attention
  â†“              â†“                â†“          â†“            â†“
5 regions    Optimized HTTP    548 files   Time series  Memory-efficient
Multi-year   Connection pools  ~150MB      Segmentation  Long sequences
```

### Ring Attention Engine

- **Memory Scaling**: O(n) vs O(nÂ²) for standard attention
- **Distributed Processing**: KV state rotation across devices
- **Energy Applications**: Multi-year sequence modeling
- **MLX Integration**: Apple Silicon optimization

## ğŸ“Š Performance Benchmarks

### Data Pipeline Performance

- **Throughput**: 4,051 RPS sustained (548 files in 5.1 minutes)
- **Coverage**: 1.2M+ records across 7 years (2019-2025)
- **Reliability**: 100% success rate, production-ready
- **API Compliance**: Well within EIA rate limits (5,000 req/hour)

### System Capabilities

- **Regions**: PACW, ERCO, CAL, TEX, MISO (5 major US grids)
- **Data Types**: Demand, generation, renewable mix
- **Storage**: Organized JSON files (~150MB total)
- **Processing**: Real-time + historical batch processing

## ğŸ”§ Technical Stack

### Data Infrastructure

- **API Clients**: EIA, CAISO with optimized HTTP connection pooling
- **Concurrency**: ThreadPoolExecutor with 5 workers
- **Storage**: Raw JSON + processed time series data
- **Monitoring**: Real-time progress and performance metrics

### ML/AI Components

- **Ring Attention**: Memory-efficient transformer architecture
- **MLX Framework**: Apple Silicon-optimized compute
- **Time Series**: Preprocessing, segmentation, normalization
- **Distributed**: Multi-device coordination and KV state rotation

## ğŸ“ Project Structure

```
â”œâ”€â”€ scripts/                          # Entry points and utilities
â”‚   â””â”€â”€ run_full_extraction_monitored.py  # Main extraction script
â”œâ”€â”€ src/core/                         # Core pipeline components
â”‚   â”œâ”€â”€ integrations/eia/             # EIA API client + data loaders
â”‚   â”œâ”€â”€ pipeline/orchestrators/       # Extraction orchestrators
â”‚   â””â”€â”€ llms/ring_attention.py        # Ring attention implementation
â”œâ”€â”€ docs/                             # Documentation and benchmarks
â”‚   â””â”€â”€ extraction_benchmark_documentation.md
â”œâ”€â”€ data/                             # Data storage (gitignored)
â”‚   â”œâ”€â”€ raw/eia/                      # Raw JSON responses
â”‚   â””â”€â”€ processed/                    # Cleaned time series data
â””â”€â”€ tests/                            # Test suites and benchmarks
```

## ğŸ“š Documentation

- **[Extraction Benchmarks](docs/extraction_benchmark_documentation.md)**: Complete performance journey and optimization details
- **[Data Architecture](docs/data_architecture_plan.md)**: Pipeline design and API optimization strategies
- **[MLX Integration](docs/mlx_integration_summary.md)**: Ring attention implementation details

## ğŸ¤ Contributing

This project welcomes contributions in:

- **Pipeline Optimization**: API client improvements, concurrency patterns
- **Ring Attention Research**: Memory efficiency, distributed communication
- **Energy Applications**: Forecasting models, grid optimization algorithms
- **Infrastructure**: Monitoring, testing, deployment automation

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

**Built for engineering teams working on high-performance data pipelines and ML research teams exploring efficient attention mechanisms for long-sequence modeling.**
